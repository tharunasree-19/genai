# -*- coding: utf-8 -*-
"""day 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18G199D_kXQ9IBnoL0nCrH6kvlT9M_dWC
"""



AL?--->is the ability of machines to imitate human intelligence such as :
thinking
learning
reasoning
decision making
problem soliving

AL= making machines "smart " like humans



ML---> is a subset of AL where machines learns from data instead of being explicity programmed
 ML= learning from experience (data),not rules

 eg:
 1,email spam detection
 2.netflix and youtube recommendation system
 3.credict card fraud detection
 4.product recommendations on amazon


 DL?-->it is subset of ML that uses neural networks with many layers to learn the complex patterns from large data.
  DL= learning using artificail brain like structures (neural networks).

  eg:
  1. face recog
  2.speech recog
  3.self driving cars
  4.image classification
  5.language translation
  6.object detection


  GEN AL -->it is a type of AI that can create new content like :
   text
   images
   music
   code
   videos
   voice



   AL -->makes machines intelligent
   ML-->machines learn from data
   DL-->machines learn using neural networks
   GenAL-->machines create new content

#1why AI became powerful now (not before )

AI existed earlier ,but only now it became practical and powerful :
3 big reasons:

reason 1 :more data (fuel of AI)
  earlier :
     very less digital data
     data was on paper or small databases
  now :
   social media
   smartphones
   sensors
   online videos ,images ,text

   A student learns better if they read more books
   Al learns better when it sees more data

    more data = better learning


  reason 2: faster computers (GPUs )
  earler :
    normal CPUs
    training will be slow
    training took months or years

 now :
 GPU and TPU
 can do miilinons of calculations at the same time
 training that took months --->now takes hours or days

 CPU is like one teacher teaching one student
 GPU is like one teacher teaching 1000 students at once .

  speed made AI practical

  reason 3 better algorithms
  earlier :
    simple rules
    poor learning methods
  now :
   deep learning
   transformers
   attention mechanism
   better optimization techniques

   AI became powerful not beacuse of one reason ,
   but because data+faster computers+algorithms improved together .--->powerful AI ssytems

#2types of AI (levels of AL )
AL classified into 3 levels based on intelligence and capability :

1. Narrow AL(weak AI)-->is designed to perform one specific task very effectively
 eg:
 chatgpt
 google maps
 face unlock
 spam filters

2.general ai(strong ai)-->is an Ai system that can think ,learn,and reason like a human and perform any intellectual task
 it can understand ,learn, and adapt across different tasks like a human
 eg:
 hypothetical ai that can :
   teach
   drive
   cook
   solve problems
   learn new skills on its own


  3.super AI(theory/sci-fi)-->is an ai that is more intelligent than humans in all aspects

  eg:sci-fic movies : like iron man

#3-types of machine learning -is mainly divided into 3 types based on how data is used
1.supervised learning -->it is type of ML where the model is trained using labeled data(input+correct output )
  the model learns by comparing its prediction with the correct answer

  eg:spam/not spam email classification
  house price prediction
  student result prediction


  supervised learning learns from examples with answers

2.unsupervised learning it is a type of ml where the model is trained on unlabeled data(no correct output given)
    the model finds patterns and gropus by itself

    eg:customer grouping
    market segmentation
    clustering similar images

     unsupervised learning finds hidden patterns without labels

  3.reinforcement learning it is a type of ml where the model learns by trial and error  usimg rwwards and penalties

       good action-->reward
       bad action-->penalty

       eg:game -playing ai(chess)
        robot learning to walk


        reinforcement learning learns by experience and feedback

#4 basic deep learning concepts

1.neuron -->it is the smallest unit of a neural network that :
  takes inpit
  prcoess it
  produce output


  a neuron is like a decision maker

2.neural network -->it is a collection of many neurons connected together to solve a probelm

   many small decisions working together make one smart decision


   eg:
   face recog
   voice recog
   image classification


  3.layers (input,hidden,output )
  layers are levels of processing in neural network

  types :
   input -->receives data (inmage ,text,numbers)
   hidden --learn patterns
   output -->gives final result




4.trabing and testing

training -->model learns from known data

testing : model is evaluated on new unseen data

5.epoch --> an epoch is one complete pass of entire traning data through the netwprk

6.loss-->measure how wrong the model prediction is

#LLM (large language model) -->are the type of gen ai model that are trained on huge amounts of text data so they can :
  1.understand human language
  2.generate new text
  3.ans questions
  4.hold conversations
  5.wrtite code,stories ,emails etcc...

  large -->means billions of parameters
  language -->text,sentence,words
  model -->trained mathematical system
     LLMs are AI systems that can read,wrte ,and talk like humans

EG:
 imagine a student who has read millions of books,articles,websites and notes

 that student :
   know grammer
   understand sentence struture
   can ans questions
   can write essays on many topics


   LLM learn the same way -->by reading massive text data , not by memorizing the data,by learning patterns ,



  #how LLMs actually work

LLMs do not think
LLMs do not understand like humans
LLMs do not have consciousness

They work by predicting the next word

eg:
 india is a capital of -----

 because during training.it saw this patterns thousands of times
   so internally ,LLMs are doing
      "given thses words", what is the most probable next word ?


LLMs do not store knowledge like a databases
 they :
   don't verify facts
   don't check truth
   don't understand meaning

they only
  predict probabilities


  eg:

  chatgpt -->chat,coding or learnin g
  google gemini-->search+ai assistant
  claude ai-->safe and long document handling
  microsoft copilot -->code +office automation


   these are applications built using LLMs ,not the model itself


   why llms are powerful?
 beacuse they can:
 understand context
 handle multiple tasks
 work across domains


one model can do :
   programming
   medical text analysis
   legal documents
   marketing content
   chatbots

#transformers -->is a neural network architecture designed to process entire sentence at once instead of reading word by word

it allows model to:
  understand context better
  train faster
  scale to huge datasets


  old way(RNN):

  reading a sentecne like :
    i am learning artifical intelligence

RNN reads :
 i -->then
 am -->then
 learning -->then
 artifical -->then
 intelligence -->then


 transformers :
  reads all words together
  looks at the relationships btw words
  understand meaning better

  like scanning the entire sentence at once instead of redaing letter by letter

#self attention -->words in a sentence look at other words in the same sentence to understand thier meaning

eg:the animal didn't cross the street because it was tired

  what does it refer to?

  it look at
   animal


   i like the movie but the ending was bad
     bad attends to :
       'ending '---right
       'movie'--wrong

       understanding a word by looking at realted words in the same sentence


      step 1: input sequence
        i love gen ai

        we break it into words(tokens )

        [I] [love] [gen] [ai]

        each word is converted into a vector(embedding )

        at this stage ,words have no context



        step 2:why we need self attention:
         without self attention :
           "gen" doesn't know it realtes to "ai"


        step 3: each word asks 3 questions(Q,K,V):
        for each word ,we create 3 vectors
         vector                             question it ans
         query(Q)                           what am i looking for ?
         key(k)                             what do i contain?
         value(V)                           what information do i have ?


         every word asks a question,checks others,and collection information from other words


         step 4: query meets keys (finding relevance )

         each words Q is comapred with keys in all words .

            how much should i pay attention to each word ?


            i love gen ai

            comapre Q(gen) with :
             k(I)
             k(love)
             k(gen)
             k(ai)
             result :
             hhigh score with ai
             low score with I
             low score with gen
             low score with love

        step 5 :convert scores to attention weights

        scores are converted into weights :
           imp words -->higher weights
           unimp words--.lower weights

           "gen":
           i     --->0.05
           love--->0.15
           gen--->0.03
           ai--->0.50

#architecture of transformers